<!DOCTYPE html>
<html>
<head>
  <title>Dimensionality Reduction Techniques</title>
  <meta charset="utf-8">
  <meta name="description" content="Dimensionality Reduction Techniques">
  <meta name="author" content="Gunnvant Singh">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>Dimensionality Reduction Techniques</h1>
    <h2>Selecting most important features</h2>
    <p>Gunnvant Singh<br/>Subject Matter Expert</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li>Why to reduce features?</li>
<li>Classical Techniques of feature reduction:

<ul>
<li>Techniques Based on Information theory/Entropy: WOE, Mutual Information</li>
<li>Classical Statistical Ideas: Chi Square, F test, Zero Variance</li>
</ul></li>
<li>Techniques from ML school:

<ul>
<li>Regularization: L1 and L2 norm</li>
</ul></li>
<li>Techniques based on Reduced Feature space:

<ul>
<li>PCA, LDA</li>
</ul></li>
</ol>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Why to reduce features?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Multi fold issue:</p>

<ul>
<li>Modelling very high dimension data sets can be computationally very expensive</li>
<li>Variables might be highly correlated: Linear Models assume there is no multidisciplinary</li>
<li>There might be some latent relationships that can be unearthed by reducing the dimensions</li>
</ul></li>
<li><p>From a modelling perspective, dimensional reduction/feature selection are synonymous</p></li>
<li><p>From a data exploration/visualization perspective, dimensionality reduction has a slightly different meaning, will see with an example</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>The DRT Zoo</h2>
  </hgroup>
  <article data-timings="">
    <iframe src='DRT_ZOO.html'></iframe>
  

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>One of the most common task while modelling is to select variables before modelling process begins.</p></li>
<li><p>In Finance, Telecom and Retail industry, where propensity models are built, sifting through relevant variables can be quiet a daunting task.</p></li>
<li><p>In most binary classification problems, Information Value or IV is most commonly used metric</p></li>
<li><p>IV has been inspired from entropy/Mutual Information</p></li>
<li><p>There are packages in R which can compute IV, while in Python, Mutual Information is computed</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Mutual Information for two Random Variables X and Y is defined as:</li>
</ul>

<p>\[\sum_{x \in X}\sum_{y \in Y} p(x,y) log\frac{p(x,y)}{p(x)p(y)};x,y \in R\] </p>

<ul>
<li><p>Why do you think this equation makes sense? (Hint: While doing variable selection we are looking at the correlation between dv and idv)</p></li>
<li><p>Can you think of bounds to this identity? When there is no relationship between two variables what will be the value of MI?</p></li>
<li><p>If you think about it, this expression is not greatly different from the idea of correlation, may be more general.</p></li>
<li><p>Both R and Python have inbuilt routines to compute mutual information.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Lets look at some code that can help us in doing feature selection based on mutual information</li>
</ul>

<pre><code class="r">setwd(&#39;/media/ramius/New Volume/Work/Jigsaw Academy/CasesFinancial/Data&#39;)
data=read.csv(&#39;Aquisition_Risk.csv&#39;)
data=data[,-c(14,15)]
data=na.omit(data)
library(entropy)
print(mi.plugin(table(data$Good_Bad,data$grade)))
</code></pre>

<pre><code>## [1] 0.005513751
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">y=data$Good_Bad
x=data[-24]
MI&lt;-function(x,y){
  MI_Store&lt;-1:length(x)
  names(MI_Store)&lt;-names(x)
  for(i in 1:length(x)){
   MI_Store[i]&lt;- mi.plugin(table(y,x[[i]]))
  }
  MI_Store=data.frame(MutualInfo=sort(MI_Store,decreasing = T))

return(MI_Store)  
}
mutual_information&lt;-MI(x=x,y=y)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <pre><code>##                   MutualInfo
## loan_status      0.193767717
## earliest_cr_line 0.192324258
## GreaterThan120   0.129159550
## revol_bal        0.092883848
## Between31_120    0.037936762
## annual_inc       0.027099639
## dti              0.015783384
## loan_amnt        0.006464012
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Here is a python routine, this feature is available in sklearn version 18
(The detailed script is available separately)</li>
</ul>

<pre><code class="python">import sklearn.feature_selection as feature_selection
MI=feature_selection.mutual_info_classif(X,y)
print MI.sort_values()
#X,y are predictor matrix and target vector

</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <pre><code>## delinq_2yrs        0.000000
## total_acc          0.001419
## dti                0.001694
## fico_range_low     0.001906
## revol_bal          0.002134
## loan_amnt          0.002955
## annual_inc         0.003029
## open_acc           0.003479
## fico_range_high    0.003740
## inq_last_6mths     0.008058
## dtype: float64
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Based on the idea of mutual information, there is a metric which is widely used in industry, called Information Value or IV</p></li>
<li><p>IV is used to screen variables for a binary classification problem</p></li>
<li><p>The way IV is defined is </p></li>
</ul>

<p>\[IV_{x}=\sum_{i=1}^{10}(bad_i-good_i)ln(\frac{bad_i}{good_i}) \]
<img src="IV.png" height='250' width='551'></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>In industry people write custom macros to compute IV. Though there are some R packages that help in computing IV out of the box. I couldn&#39;t locate any python implementation save for an obscure code on some github repository.</p></li>
<li><p>Traditionally people used to work with VBA macros or SQL codes to compute IVs</p></li>
</ul>

<pre><code class="r">library(smbinning)
data$Target&lt;-ifelse(data$Good_Bad==&#39;Good&#39;,1,0)
head(smbinning(data,y=&quot;Target&quot;,x=&quot;loan_amnt&quot;,p=0.05)$ivtable,2)
</code></pre>

<pre><code>##   Cutpoint CntRec CntGood CntBad CntCumRec CntCumGood CntCumBad PctRec
## 1  &lt;= 5225  20651   19213   1438     20651      19213      1438 0.1632
## 2  &lt;= 9725  28073   26681   1392     48724      45894      2830 0.2219
##   GoodRate BadRate    Odds LnOdds     WoE     IV
## 1   0.9304  0.0696 13.3609 2.5923 -0.3864 0.0291
## 2   0.9504  0.0496 19.1674 2.9532 -0.0255 0.0001
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="slide-13" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <p>Here is a python code snippet</p>

<pre><code class="python">import information_value as information_value
import numpy as np
x=np.array(X[&#39;loan_amnt&#39;])
y=np.array(y)
woe=information_value.WOE()
print woe.woe_single_x(x=woe.discrete(x),y=y)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Classical Techniques</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>There are still some metrics that are can be used to do feature selection, eg doing a chi square test of factor independence to see if two categorical variables are related.</p></li>
<li><p>If there is very little variance in a feature then it is indicative of non dependence of that feature.</p></li>
<li><p>Its easy to find routines that can implement chi square test and compute variance</p></li>
<li><p>Implementation is left as an exercise</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Techniques from ML school: L1 and L2 norms</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>These techniques have been inspired from ML school, the idea is to do feature selection as well as model building simultaneously</p></li>
<li><p>Originally developed to handle over fitting in linear models, these can still be used as an aid to feature selection</p></li>
<li><p>Mostly used to reduce dimensionless when data will naturally tend to have very large number of dimensions eg. Image data</p></li>
<li><p>The idea is to add a  penalty term in the original cost function, the penalty can be an L1 (Lasso) penalty or L2 (Ridge) penalty</p></li>
<li><p>L1 (Lasso), penalty can make some to the coefficients exactly zero and hence can aid in feature selection</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Techniques from ML school: L1 and L2 norms</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>This is how the cost functions are modified:</li>
</ul>

<p>\[C(\beta)+\lambda\sum_{i}|\beta_i|\]</p>

<ul>
<li><p>Here \(C(\beta)\) can be either RSS or Logistic cost function and \(\lambda\sum_{i}|\beta_i|\) is the L1 penalty or L1 norm.</p></li>
<li><p>To see the intuition behind why this cost function works, let&#39;s look at a modified linear regression cost function.</p></li>
</ul>

<p>\[\sum_{i}(y_i-(\beta_0+x_1\beta_1+..+x_n\beta_n))^2+\lambda\sum_{i}|\beta_1+\beta_2+..+\beta_n|\]</p>

<ul>
<li>L2 penalty is different in the sense that instead of summing up absolute values of betas, we sum up their squares:</li>
</ul>

<p>\[C(\beta)+\lambda\sum_{i}\beta^2_{i}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Techniques from ML school: L1 and L2 norms</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Let&#39;s look at some R and Python code implementing L1 penalty. The R code will be demoed with MNIST data set, we will use 8 by 8 pixel grey scale images.</li>
</ul>

<pre><code class="r">library(glmnet)
X=read.csv(&#39;mnist_x.csv&#39;,header=T)
Y=read.csv(&#39;mnist_y.csv&#39;,header=F,col.names = &#39;Target&#39;)
y=ifelse(Y$Target&gt;7,1,0)
x=as.matrix(X)
mod&lt;-cv.glmnet(x,y,family=&#39;binomial&#39;,alpha=1)## Does a cv search over a grid of lambdas
head(coef(mod,s=mod$lambda.min),3)
</code></pre>

<pre><code>## 3 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                      1
## (Intercept) -6.5052806
## X0           .        
## X1          -0.1342458
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Techniques from ML school: L1 and L2 norms</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Let&#39;s look at a python code snippet, I am using a standard sklearn implementation</li>
</ul>

<pre><code class="python">import sklearn.linear_model as linear_model

clf=linear_model.LogisticRegressionCV(fit_intercept=True,cv=10,penalty=&#39;l1&#39;,n_jobs=-1,
solver=&#39;liblinear&#39;)

def get_target(x):
    if x&gt;7:
        return 1
    else:
        return 0

y=mnist_y.map(get_target)

mod=clf.fit(mnist_x,y)

</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Another set of techniques that are used, rely on changing the original feature space and projecting the features in new space, this new space has less dimensions compared to the original space</p></li>
<li><p>Lets try to understand the intuition behind what it means to reduce the dimensions</p></li>
</ul>

<p><img src="assets/fig/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Let&#39;s look at another scenario: (Which &quot;component&quot; will you choose?, Guiding principle is?)</li>
</ul>

<p><img src='pca1.png'></p>

<p><img src='pca2.jpeg'></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ol>
<li><p>Here are a few facts that are worth mentioning:</p>

<ul>
<li>Mathematically, choosing new &quot;components&quot; which captures maximum variance is akin to finding the Eigen Vectors of variance-covariance matrix of the data</li>
<li>Eigen value of each of these Eigen Vectors measures the variance captured by each eigen vector/principal component</li>
<li>Eigen values (\(\lambda\)) and Eigen vectors (\(e_i\)) can be computed if Variance-Covariance matrix (\(\Sigma\)) is known by making use of following equations:</li>
</ul>

<p>\[det(\Sigma-\lambda I)=0\]</p></li>
</ol>

<p>\[\Sigma e_i=\lambda_ie_i\]</p>

<ul>
<li><p>The computed eigen vector \(e_i\), will look something like this</p>

<p>\[e_i=(\beta_{i1},\beta_{i2},...,\beta_{in})\]</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The principal components are nothing but linear combination of original features. 
\[PC_1=\beta_{11}var_1+\beta_{12}var_2+...+\beta_{1n}var_n\]
\[PC_2=\beta_{21}var_1+\beta_{22}var_2+...+\beta_{2n}var_n\]</li>
<li><p>The \(\beta_{11}, \beta_{12}\) etc are loadings and can be used to figure out which original variables characterize the principal component</p></li>
<li><p>One can project the points in a particular dimension by scoring that point on the respective principal component.</p></li>
</ul>

<p><img src='pca4.jpg' height='250' width='800'></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>How many components will you choose?</li>
</ul>

<p><img src='pca3.png' height='400' width='400'></p>

<ul>
<li>How did you choose?</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Can we formalize the process of how many Principal Components to choose?</li>
<li>What was the guiding principal to choose?</li>
<li>Can we compute any measure to guide us in our endeavor? (Go back a few slides, you will find the answer)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <article data-timings="">
    <ul>
<li>Eigen value will measure the variance explained by an eigen vector (principal components)</li>
</ul>

<p><img src='pca5.png'></p>

<p><img src='pca6.png'></p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>There are some data prep issues that have to be kept in mind</p>

<ul>
<li>Features should be numeric only.</li>
<li>Features should be scaled before being fed into PCA method.</li>
</ul></li>
<li><p>There are some limitations as well to the use of PCA:</p>

<ul>
<li>There is no guarantee that a supervised learning algorithm will do a good job if only principal components are used as predictors. </li>
<li>There are methods like LDA (for linear classifiers) and PLS (for linear regressors) that create low dimensional features by keeping into account the target variable as well.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>PCA and cousins: LDA</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>PCA vs LDA</li>
</ul>

<p><img src='WhyLDA.jpg' height='500'></p>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Agenda'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Why to reduce features?'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='The DRT Zoo'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Classical Techniques'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Classical Techniques'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Classical Techniques'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Classical Techniques'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Classical Techniques'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Classical Techniques'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Classical Techniques'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Classical Techniques'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Classical Techniques'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Classical Techniques'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Classical Techniques'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Techniques from ML school: L1 and L2 norms'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Techniques from ML school: L1 and L2 norms'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Techniques from ML school: L1 and L2 norms'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Techniques from ML school: L1 and L2 norms'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='PCA and cousins'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='PCA and cousins'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='PCA and cousins'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='PCA and cousins'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='PCA and cousins'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='PCA and cousins'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='NA'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='PCA and cousins'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='PCA and cousins: LDA'>
         27
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>